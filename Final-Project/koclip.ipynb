{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/CLIP/issues/175 요거 참고해서 배치작업중..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os.path import join as pathjoin\n",
    "from typing import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import requests\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "model = AutoModel.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"koclip/koclip-base-pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'train_path': '/kovar-vol/kovar/dataset/train.json',\n",
    "    'test_path': '/kovar-vol/kovar/dataset/test.json',\n",
    "    'image_path': '/kovar-vol/images/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image size:  torch.Size([3, 480, 640])\n",
      "input_ids torch.Size([3, 7])\n",
      "token_type_ids torch.Size([3, 7])\n",
      "attention_mask torch.Size([3, 7])\n",
      "pixel_values torch.Size([1, 3, 224, 224])\n",
      "tensor([[-2.9678, -0.1690,  0.2926]], grad_fn=<PermuteBackward0>) tensor([[-2.9678],\n",
      "        [-0.1690],\n",
      "        [ 0.2926]], grad_fn=<MulBackward0>)\n",
      "자동차 tensor(0.5993, grad_fn=<UnbindBackward0>)\n",
      "쳇바퀴를 달리는 햄스터 tensor(0.3777, grad_fn=<UnbindBackward0>)\n",
      "강아지와 강아지 주인 tensor(0.0230, grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
    "ToTensor = torchvision.transforms.ToTensor()\n",
    "image_tensor = ToTensor(image)\n",
    "print('original image size: ', image_tensor.shape)\n",
    "text = [\"강아지와 강아지 주인\", \"쳇바퀴를 달리는 햄스터\", \"자동차\"]\n",
    "\n",
    "# inputs = processor(\n",
    "#     text=text,\n",
    "#     images=image, \n",
    "#     return_tensors=\"pt\", # could also be \"pt\" \n",
    "#     padding=True\n",
    "# )\n",
    "img = []\n",
    "\n",
    "for k, v in inputs.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits_per_image, outputs.logits_per_text)\n",
    "probs = torch.nn.functional.softmax(outputs.logits_per_image, dim=1)\n",
    "\n",
    "for idx, prob in sorted(enumerate(*probs), key=lambda x: x[1], reverse=True):\n",
    "    print(text[idx], prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalDataset(Dataset):\n",
    "    dataset_path: str = None\n",
    "    image_path: str = None\n",
    "    transform: torchvision.transforms = None\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        ## set up train/test/image paths\n",
    "        self.dataset_path = configs['train_path'] if self.dataset_path is None else self.dataset_path\n",
    "        self.image_path = configs['image_path'] if self.image_path is None else self.image_path\n",
    "        self.dataset = pd.read_json(self.dataset_path, lines=True)\n",
    "        \n",
    "        return None\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    \n",
    "    def _return_hypothesises(self, data_sample: pd.Series) -> List[str]:\n",
    "        obs1, obs2 = data_sample[5], data_sample[6]\n",
    "        hyp0, hyp1, hyp2 = data_sample[12], data_sample[13], data_sample[14]\n",
    "        \n",
    "        return [obs1 +  \" \" +hyp0 + \" \" + obs2, obs1 + \" \" + hyp1 + \" \" + obs2, obs1 + \" \" + hyp2 + \" \" + obs2]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx) -> Tuple[List[str], PIL.Image.Image, int]:\n",
    "        data_sample = self.dataset.iloc[idx, :]\n",
    "        \n",
    "        ## Get image and transform\n",
    "        image_id = data_sample[4]\n",
    "        image = PIL.Image.open(pathjoin(self.image_path, image_id[:3], f\"{image_id}.jpg\"))\n",
    "        image = PIL.ImageOps.exif_transpose(image)  ## prevent Image rotation by default camera settings\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        ## Get Texts\n",
    "        hyps_list = self._return_hypothesises(data_sample)\n",
    "        \n",
    "        inputs = processor(text=hyps_list,\n",
    "                           images=image,\n",
    "                           return_tensors='pt',\n",
    "                           padding='max_length',\n",
    "                           max_length=70)\n",
    "        \n",
    "        ## TODO: labels to be dealt with\n",
    "        # label = data_sample[-1]\n",
    "        \n",
    "        return inputs   ## inputs are dictionary {input_ids: torch.Tensor (shape: num_hyps, padded sequence length), \n",
    "                        ##                        token_type_ids: torch.Tensor (shape: num_hyps, padded sequence length),\n",
    "                        ##                        attention_mask: torch.Tensor (shape: num_hyps, padded sequence length),\n",
    "                        ##                        pixel_values: torch.Tensor (shape: 1, color channels, height, width)\n",
    "                        ##                       }\n",
    "                        ## ?? pixel values의 첫번째 차원이 아마 배치 수인 것 같은데, 이거 squeeze하면 안받아준다....\n",
    "                        ## ?? 라벨은 어떻게 처리하지??? visualBERT의 경우에는 inputs dict에서 label도 받았는데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([3, 70])\n",
      "token_type_ids torch.Size([3, 70])\n",
      "attention_mask torch.Size([3, 70])\n",
      "pixel_values torch.Size([1, 3, 224, 224])\n",
      "input_ids torch.Size([3, 70])\n",
      "token_type_ids torch.Size([3, 70])\n",
      "attention_mask torch.Size([3, 70])\n",
      "pixel_values torch.Size([1, 3, 224, 224])\n",
      "input_ids torch.Size([2, 3, 70])\n",
      "token_type_ids torch.Size([2, 3, 70])\n",
      "attention_mask torch.Size([2, 3, 70])\n",
      "pixel_values torch.Size([2, 1, 3, 224, 224])\n",
      "input_ids torch.Size([2, 3, 70])\n",
      "token_type_ids torch.Size([2, 3, 70])\n",
      "attention_mask torch.Size([2, 3, 70])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "resize_and_normalize = torchvision.transforms.Compose([torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                          torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]), ## mean & std value are convention calculated from ImageNet.\n",
    "                                                          torchvision.transforms.ToPILImage(),\n",
    "                                                        ])\n",
    "\n",
    "multimodal_dataset = MultimodalDataset(dataset_path=configs['test_path'], transform=resize_and_normalize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "it = enumerate(multimodal_dataset)\n",
    "\n",
    "i, inputs1 = next(it)\n",
    "# print(i, inputs1)\n",
    "i, inputs2 = next(it)\n",
    "# print(i, inputs2)\n",
    "\n",
    "for k, v in inputs1.items():\n",
    "  print(k, v.shape)\n",
    "  \n",
    "for k, v in inputs2.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "for k, v in inputs2.items():\n",
    "  inputs1[k] = torch.stack((inputs1[k], v),axis=0)\n",
    "\n",
    "for k, v in inputs1.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "\n",
    "\n",
    "inputs1['pixel_values'] = inputs1['pixel_values'].squeeze()\n",
    "for k, v in inputs1.items():\n",
    "  print(k, v.shape)\n",
    "  \n",
    "  \n",
    "# outputs = model(**inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1['pixel_values'].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'input_ids': tensor([[[    0, 16221,  2069,  1511,  2073,  3651,  7285,  1408,   553,  1485,\n",
      "           2138,   571,  2088,  1513,  2359,  2062,    18,  4836,  2138,  1163,\n",
      "           3611,  2031,  2073,  6435,  2200,  6339,  2116,  2112,  7306,  7953,\n",
      "           2031,  2069,  6263,  3670,  2371,  2062,    18,  3651,  2116,  1408,\n",
      "            553,  1485,  2170,  1418,  3031, 26481,  2138,     3,    18,     2,\n",
      "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "         [    0, 16221,  2069,  1511,  2073,  3651,  7285,  1408,   553,  1485,\n",
      "           2138,   571,  2088,  1513,  2359,  2062,    18,   636,  1570,  1891,\n",
      "           3651,  2116,  6989,  2205,  2318,  1123,  2069,     3,    18,  3651,\n",
      "           2116,  1408,   553,  1485,  2170,  1418,  3031, 26481,  2138,     3,\n",
      "             18,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "         [    0, 16221,  2069,  1511,  2073,  3651,  7285,  1408,   553,  1485,\n",
      "           2138,   571,  2088,  1513,  2359,  2062,    18,   636,  1570,  1891,\n",
      "           3651,  2116,  6529,  2069,  1689,  2307,   927,  2359,  2062,    18,\n",
      "           3651,  2116,  1408,   553,  1485,  2170,  1418,  3031, 26481,  2138,\n",
      "              3,    18,     2,     1,     1,     1,     1,     1,     1,     1,\n",
      "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]]), 'token_type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0]]]), 'pixel_values': tensor([[[[-1.4273, -1.4273, -1.4273,  ...,  0.1931,  0.0617, -0.0696],\n",
      "          [-1.2375, -1.1791, -1.3689,  ...,  0.2515,  0.1201,  0.0033],\n",
      "          [-0.5952,  0.4121, -0.1572,  ...,  0.3829,  0.2515,  0.1201],\n",
      "          ...,\n",
      "          [ 1.3464,  1.5946,  1.8573,  ...,  1.4048,  1.5362,  1.7260],\n",
      "          [ 1.2880,  1.7844,  1.7844,  ...,  1.4048,  1.5362,  1.4778],\n",
      "          [ 1.3464,  1.4778,  1.5946,  ...,  1.5362,  1.7260,  1.7844]],\n",
      "\n",
      "         [[-0.9117, -0.9117, -0.9117,  ...,  0.1689,  0.0338, -0.1012],\n",
      "          [-0.7766, -0.7766, -0.9117,  ...,  0.2289,  0.0939, -0.0412],\n",
      "          [-0.1763,  0.7692,  0.2289,  ...,  0.3640,  0.2289,  0.0939],\n",
      "          ...,\n",
      "          [-1.1368, -0.8666, -0.6565,  ..., -1.0017, -0.7916, -0.5215],\n",
      "          [-1.1968, -0.6565, -0.6565,  ..., -0.9267, -0.7316, -0.7916],\n",
      "          [-1.0617, -0.9267, -0.7916,  ..., -0.7916, -0.5965, -0.5215]],\n",
      "\n",
      "         [[-0.1009, -0.1009, -0.0440,  ...,  1.2216,  1.0936,  0.9656],\n",
      "          [ 0.0271,  0.0271, -0.0440,  ...,  1.2785,  1.1647,  1.0936],\n",
      "          [ 0.5248,  1.2216,  0.8377,  ...,  1.4065,  1.2785,  1.1647],\n",
      "          ...,\n",
      "          [-0.1720,  0.0840,  0.2688,  ...,  0.4679,  0.7097,  0.9656],\n",
      "          [-0.1720,  0.3399,  0.3399,  ...,  0.5248,  0.7808,  0.7097],\n",
      "          [-0.0440,  0.0840,  0.2120,  ...,  0.5959,  0.8377,  0.8377]]]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(multimodal_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(i, batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py:364\u001b[0m, in \u001b[0;36mVisionTextDualEncoderModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, token_type_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    355\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mreturn_dict\n\u001b[1;32m    357\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_model(\n\u001b[1;32m    358\u001b[0m     pixel_values\u001b[39m=\u001b[39mpixel_values,\n\u001b[1;32m    359\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    360\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    361\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_model(\n\u001b[1;32m    365\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    366\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    367\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    368\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    369\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    370\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    371\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    374\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]  \u001b[39m# pooler_output\u001b[39;00m\n\u001b[1;32m    375\u001b[0m image_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_projection(image_embeds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/roberta/modeling_roberta.py:798\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 798\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[1;32m    799\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    801\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "## TODO\n",
    "# def collate_for_multiple_choice(self, features_list: List[Dict]) -> dict:\n",
    "#     return None\n",
    "    \n",
    "\n",
    "multimodal_dataloader = DataLoader(multimodal_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(multimodal_dataloader):\n",
    "    print(i, batch)\n",
    "    model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 70])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m a, b, c, d \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "a, b, c, d = 1, 2, 3, 4, 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
