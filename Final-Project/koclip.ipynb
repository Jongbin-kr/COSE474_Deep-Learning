{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/CLIP/issues/175 요거 참고해서 배치작업중..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os.path import join as pathjoin\n",
    "from typing import *\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "model = AutoModel.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"koclip/koclip-base-pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'train_path': '/kovar-vol/kovar/dataset/train.json',\n",
    "    'test_path': '/kovar-vol/kovar/dataset/test.json',\n",
    "    'image_path': '/kovar-vol/images/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image size:  torch.Size([3, 480, 640])\n",
      "(3, 224, 224)\n",
      "input_ids torch.Size([3, 7])\n",
      "token_type_ids torch.Size([3, 7])\n",
      "attention_mask torch.Size([3, 7])\n",
      "pixel_values torch.Size([1, 3, 224, 224])\n",
      "tensor([[-2.9678, -0.1690,  0.2926]], grad_fn=<PermuteBackward0>) tensor([[-2.9678],\n",
      "        [-0.1690],\n",
      "        [ 0.2926]], grad_fn=<MulBackward0>)\n",
      "자동차 tensor(0.5993, grad_fn=<UnbindBackward0>)\n",
      "쳇바퀴를 달리는 햄스터 tensor(0.3777, grad_fn=<UnbindBackward0>)\n",
      "강아지와 강아지 주인 tensor(0.0230, grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
    "ToTensor = torchvision.transforms.ToTensor()\n",
    "image_tensor = ToTensor(image)\n",
    "print('original image size: ', image_tensor.shape)\n",
    "text = [\"강아지와 강아지 주인\", \"쳇바퀴를 달리는 햄스터\", \"자동차\"]\n",
    "\n",
    "inputs = processor(\n",
    "    text=text,\n",
    "    images=image, \n",
    "    return_tensors=\"pt\", # could also be \"pt\" \n",
    "    padding=True\n",
    ")\n",
    "\n",
    "image_features = image_processor(image)\n",
    "print(image_features['pixel_values'][0].shape)\n",
    "imgs = []\n",
    "\n",
    "for k, v in inputs.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits_per_image, outputs.logits_per_text)\n",
    "probs = torch.nn.functional.softmax(outputs.logits_per_image, dim=1)\n",
    "\n",
    "for idx, prob in sorted(enumerate(*probs), key=lambda x: x[1], reverse=True):\n",
    "    print(text[idx], prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalDataset(Dataset):\n",
    "    dataset_path: str = None\n",
    "    image_path: str = None\n",
    "    transform: torchvision.transforms = None\n",
    "    \n",
    "    def __post_init__(self) -> None:\n",
    "        ## set up train/test/image paths\n",
    "        self.dataset_path = configs['train_path'] if self.dataset_path is None else self.dataset_path\n",
    "        self.image_path = configs['image_path'] if self.image_path is None else self.image_path\n",
    "        self.dataset = pd.read_json(self.dataset_path, lines=True)\n",
    "        \n",
    "        return None\n",
    "\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    \n",
    "    def _return_hypothesises(self, data_sample: pd.Series) -> List[str]:\n",
    "        obs1, obs2 = data_sample[5], data_sample[6]\n",
    "        hyp0, hyp1, hyp2 = data_sample[12], data_sample[13], data_sample[14]\n",
    "        \n",
    "        return [obs1 +  \" \" +hyp0 + \" \" + obs2, obs1 + \" \" + hyp1 + \" \" + obs2, obs1 + \" \" + hyp2 + \" \" + obs2]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx) -> Tuple[List[str], PIL.Image.Image, int]:\n",
    "        data_sample = self.dataset.iloc[idx, :]\n",
    "        \n",
    "        ## Get image and transform\n",
    "        image_id = data_sample[4]\n",
    "        image = Image.open(pathjoin(self.image_path, image_id[:3], f\"{image_id}.jpg\"))\n",
    "        image = ImageOps.exif_transpose(image)  ## prevent Image rotation by default camera settings\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        ## Get Texts\n",
    "        hyps_list = self._return_hypothesises(data_sample)\n",
    "        \n",
    "        inputs = processor(text=hyps_list,\n",
    "                           images=image,\n",
    "                           return_tensors='pt',\n",
    "                           padding='max_length',\n",
    "                           max_length=70)\n",
    "        \n",
    "        ## TODO: labels to be dealt with\n",
    "        # label = data_sample[-1]\n",
    "        \n",
    "        return inputs   ## inputs are dictionary {input_ids: torch.Tensor (shape: num_hyps, padded sequence length), \n",
    "                        ##                        token_type_ids: torch.Tensor (shape: num_hyps, padded sequence length),\n",
    "                        ##                        attention_mask: torch.Tensor (shape: num_hyps, padded sequence length),\n",
    "                        ##                        pixel_values: torch.Tensor (shape: 1, color channels, height, width)\n",
    "                        ##                       }\n",
    "                        ## ?? pixel values의 첫번째 차원이 아마 배치 수인 것 같은데, 이거 squeeze하면 안받아준다....\n",
    "                        ## ?? 라벨은 어떻게 처리하지??? visualBERT의 경우에는 inputs dict에서 label도 받았는데..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'from_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m     get_dataset \u001b[39m=\u001b[39m JsonToDataset()\n\u001b[0;32m--> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m     train_set \u001b[39m=\u001b[39m get_dataset(paths[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_path\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m     test_set \u001b[39m=\u001b[39m get_dataset(paths[\u001b[39m\"\u001b[39m\u001b[39mtest_path\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m     \u001b[39m# max_seq_length = get_dataset.get_max_seq_length()\u001b[39;00m\n",
      "\u001b[1;32m/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mCLUE1\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_image_paths)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39minput_prompt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_texts(row), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_pandas(df)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Dataset' has no attribute 'from_pandas'"
     ]
    }
   ],
   "source": [
    "paths = {\n",
    "    ## dataset & image path\n",
    "    \"train_path\": \"/kovar-vol/kovar/dataset/train.json\",\n",
    "    \"test_path\": \"/kovar-vol/kovar/dataset/test.json\",\n",
    "    \"image_path\": \"/kovar-vol/images\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class JsonToDataset:\n",
    "    '''\n",
    "    Convert json files of train/test set to dataset object of hugginface datasets\n",
    "    - add 'image_path'and 'input_prompt' columns to original DataFrame\n",
    "    - 'image_path' : abs_path for each images\n",
    "    - 'input_prompt' : list of str, ['{obs1}[sep]{hyp0}[sep]{obs2}', '{obs1}[sep]{hyp1}[sep]{obs2}', ...]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.tokenizer = processor.tokenizer\n",
    "        # # Not used now\n",
    "        # self.max_seq_length = 0\n",
    "\n",
    "    def __call__(self, json_path: str) -> Dataset:\n",
    "        df = pd.read_json(json_path, lines=True)\n",
    "        df[\"image_path\"] = df[\"CLUE1\"].map(self._get_image_paths)\n",
    "        df[\"input_prompt\"] = df.apply(lambda row: self._format_texts(row), axis=1)\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "        return dataset\n",
    "\n",
    "    def _get_image_paths(self, image_id: str) -> str:\n",
    "        path = os.path.join(paths[\"image_path\"], image_id[:3], f\"{image_id}.jpg\")\n",
    "        return path\n",
    "\n",
    "    def _format_texts(self, row: pd.DataFrame) -> List[str]:\n",
    "        sep_token = self.tokenizer.sep_token\n",
    "        obs1 = row[\"OBS1\"]\n",
    "        obs2 = row[\"OBS2\"]\n",
    "        hyps = row.loc[\"hyp0\":\"hyp2\"]\n",
    "        text_list = list()\n",
    "        for hyp in hyps.values:\n",
    "            prompt_format = sep_token.join([obs1, hyp, obs2])\n",
    "            # self.max_seq_length = max(len(prompt_format), self.max_seq_length)\n",
    "            text_list.append(prompt_format)\n",
    "        return text_list\n",
    "\n",
    "    # def get_max_seq_length(self):\n",
    "    #     return self.max_seq_length\n",
    "\n",
    "\n",
    "class MultimodalDataset:\n",
    "    '''\n",
    "    Dataset for KoVAR task based on koCLIP\n",
    "    - return image, text, label when __getitem__ is called\n",
    "    - image : an image (PIL.Image.Image)\n",
    "    - text : List of str, several choices of hypothesis and observations\n",
    "    - label : the index of sentence that contains a plausible hypothesis in the 'text'.\n",
    "    '''\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.tokenizer = processor.tokenizer\n",
    "\n",
    "        self.image_paths = dataset[\"image_path\"]\n",
    "        self.texts = dataset[\"input_prompt\"]\n",
    "        self.labels = dataset[\"label\"]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        image = self._load_image(idx)\n",
    "        text = self._load_texts(idx)\n",
    "        label = self._load_label(idx)\n",
    "        return image, text, label\n",
    "\n",
    "    def _load_image(self, idx: int) -> Image.Image:\n",
    "        path = self.image_paths[idx]\n",
    "        image = Image.open(path)\n",
    "        return image\n",
    "\n",
    "    def _load_texts(self, idx: int) -> List[str]:\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def _load_label(self, idx: int) -> int:\n",
    "        return self.labels[idx]\n",
    "\n",
    "\n",
    "def collate_fn(examples: List[tuple]):\n",
    "    \"\"\"\n",
    "    example[0] = images\n",
    "    example[1] = texts\n",
    "    example[2] = labels\n",
    "    \"\"\"\n",
    "    examples = list(filter(lambda x: x is not None, examples))\n",
    "    \n",
    "\n",
    "    # make labels\n",
    "    num_hyp = 3  # temperally fixed\n",
    "    labels_idx = [example[2] for example in examples]\n",
    "    labels = np.zeros((len(examples), num_hyp))\n",
    "    for i, label in enumerate(labels_idx):\n",
    "        labels[i][label] = 1\n",
    "    print(labels)\n",
    "    \n",
    "    # make list of dicts\n",
    "    inputs = []   # inputs: List[dict]\n",
    "    lengths = []  # lengths: List[int]\n",
    "    for example in examples:\n",
    "        input = processor(images=example[0], text=example[1], return_tensors='pt', padding=True )\n",
    "        lengths.append(input[\"input_ids\"].shape[1])  # 3 X N\n",
    "        inputs.append(input)\n",
    "    \n",
    "    # dynamic padding in batch\n",
    "    max_length = max(lengths)\n",
    "    for idx, input in enumerate(inputs):\n",
    "        length = lengths[idx]\n",
    "        num_pad = max_length - length\n",
    "\n",
    "        pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "        input['input_ids'] = np.pad(input['input_ids'], ((0, 0), (0, num_pad)), 'constant', constant_values=pad_token_id)\n",
    "        input['token_type_ids'] = np.pad(input['token_type_ids'], ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "        input['attention_mask'] = np.pad(input['attention_mask'], ((0, 0), (0, num_pad)), 'constant', constant_values=0)\n",
    "    \n",
    "    # merge to 1 dict\n",
    "    padded_inputs = dict()\n",
    "    keys = ['pixel_values','input_ids', 'attention_mask', 'token_type_ids']\n",
    "    for key in keys:\n",
    "        if key == 'pixel_values':\n",
    "            padded_inputs[key] = torch.stack([input[key][0] for input in inputs])\n",
    "        else:\n",
    "            padded_inputs[key] = torch.tensor([input[key] for input in inputs])\n",
    "\n",
    "\n",
    "    return padded_inputs, labels\n",
    "\n",
    "\n",
    "def get_data_loader(dataset, batch_size):\n",
    "    data_loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_dataset = JsonToDataset()\n",
    "    train_set = get_dataset(paths[\"train_path\"])\n",
    "    test_set = get_dataset(paths[\"test_path\"])\n",
    "\n",
    "    # max_seq_length = get_dataset.get_max_seq_length()\n",
    "    batch_size = 4\n",
    "\n",
    "    train_set = MultimodalDataset(train_set)\n",
    "    test_set = MultimodalDataset(test_set)\n",
    "\n",
    "    # print(train_set.__getitem__(1))\n",
    "\n",
    "    train_loader = get_data_loader(train_set, batch_size)\n",
    "    test_loader = get_data_loader(test_set, batch_size)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        print(batch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1['pixel_values'].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([2, 3, 70])\n",
      "token_type_ids torch.Size([2, 3, 70])\n",
      "attention_mask torch.Size([2, 3, 70])\n",
      "pixel_values torch.Size([2, 1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## TODO\n",
    "# def collate_for_multiple_choice(self, features_list: List[Dict]) -> dict:\n",
    "#     return None\n",
    "\n",
    "resize_and_normalize = torchvision.transforms.Compose([torchvision.transforms.Resize((224,224)),\n",
    "                                                          torchvision.transforms.ToTensor(),\n",
    "                                                          torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]), ## mean & std value are convention calculated from ImageNet.\n",
    "                                                          torchvision.transforms.ToPILImage(),\n",
    "                                                        ])\n",
    "\n",
    "multimodal_dataset = MultimodalDataset(dataset_path=configs['test_path'], transform=resize_and_normalize)   \n",
    "\n",
    "multimodal_dataloader = DataLoader(multimodal_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for i, batch in enumerate(multimodal_dataloader):\n",
    "    # print(i, batch)\n",
    "    for k, v in batch.items():\n",
    "      print(k, v.shape)\n",
    "    \n",
    "    for text_keys in ['input_ids', 'token_type_ids', 'attention_mask']: ## preprocess batch\n",
    "      batch[text_keys] = batch[text_keys].reshape(-1, batch[text_keys].shape[2])  ## batch x num_hyps x sequence lenght -> (batch * num_hyps) x sequence length\n",
    "    batch['pixel_values'] = batch['pixel_values'].squeeze()\n",
    "      \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([6, 70])\n",
      "token_type_ids torch.Size([6, 70])\n",
      "attention_mask torch.Size([6, 70])\n",
      "pixel_values torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for k, v in batch.items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "output = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2338, -0.8873, -1.0910,  0.2682, -0.3743, -0.0649],\n",
       "        [-1.5100, -1.0999, -1.3481,  0.2801, -0.4611, -0.0920]],\n",
       "       grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['logits_per_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multimodal_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m it \u001b[39m=\u001b[39m \u001b[39menumerate\u001b[39m(multimodal_dataset)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m i, inputs1 \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(it)\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a6f6e67696e622d6b6f636c6970227d@ssh-remote%2Blili/kovar-vol/kovar/Multimodal/jongbin-koclip/Final-Project/koclip.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print(i, inputs1)\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multimodal_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "it = enumerate(multimodal_dataset)\n",
    "\n",
    "i, inputs1 = next(it)\n",
    "# print(i, inputs1)\n",
    "i, inputs2 = next(it)\n",
    "# print(i, inputs2)\n",
    "\n",
    "for k, v in inputs1.items():\n",
    "  print(k, v.shape)\n",
    "  \n",
    "for k, v in inputs2.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "for k, v in inputs2.items():\n",
    "  inputs1[k] = torch.stack((inputs1[k], v),axis=0)\n",
    "\n",
    "for k, v in inputs1.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "\n",
    "\n",
    "inputs1['pixel_values'] = inputs1['pixel_values'].squeeze()\n",
    "for k, v in inputs1.items():\n",
    "  print(k, v.shape)\n",
    "  \n",
    "  \n",
    "# outputs = model(**inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 70])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from preprocess import JsonToDataset, MultimodalDataset, get_data_loader\n",
    "\n",
    "# paths\n",
    "paths = {\n",
    "    ## dataset & image path\n",
    "    \"train_path\": \"/kovar-vol/kovar/dataset/train.json\",\n",
    "    \"test_path\": \"/kovar-vol/kovar/dataset/test.json\",\n",
    "    \"image_path\": \"/kovar-vol/images\",\n",
    "    }   \n",
    "\n",
    "model_checkpoint = \"koclip/koclip-base-pt\"\n",
    "processor = AutoProcessor.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    get_dataset = JsonToDataset()\n",
    "    # train_set = get_dataset(paths[\"train_path\"])\n",
    "    test_set = get_dataset(paths[\"test_path\"])\n",
    "\n",
    "    batch_size = 8\n",
    "\n",
    "    test_set = MultimodalDataset(test_set)\n",
    "    test_loader = get_data_loader(test_set, batch_size)\n",
    "\n",
    "    num_correct = 0\n",
    "    for batched_inputs, labels in test_loader:\n",
    "        outputs = model(**batched_inputs)\n",
    "        logits_per_image = outputs['logits_per_image']\n",
    "        \n",
    "        ## 1) get proper outputs from 4*12 tensor by proper indexing\n",
    "        indices = torch.arange(logits_per_image.shape[1]).reshape(batch_size,3)      ## indices: torch.tensor([[0, 1, 2],[3,4,5],[6,7,8],[9,10,11]])\n",
    "        logits_per_image = torch.gather(input=logits_per_image,dim= 1, index = indices)\n",
    "\n",
    "        # print(logits_per_image)\n",
    "        \n",
    "        ## 2) convert logits_per_image outputs into one-hot like \n",
    "        one_hot_outputs = F.one_hot(logits_per_image.argmax(dim=1)).detach().numpy()\n",
    "        break\n",
    "        \n",
    "    #     ## 3) calculate the right outpus comparing to labels\n",
    "    #     num_correct_in_batch = (one_hot_outputs == labels).all(axis=1).sum()\n",
    "    #     print(num_correct)\n",
    "    #     num_correct += num_correct_in_batch\n",
    "    #     print(num_correct_in_batch, num_correct)\n",
    "        \n",
    "    \n",
    "    # print(f'accuracy: {num_correct / len(test_loader)}')\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 68])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_inputs['input_ids'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
