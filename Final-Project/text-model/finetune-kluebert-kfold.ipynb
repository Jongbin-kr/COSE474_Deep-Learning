{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsing cuda with 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import argparse, sys, os, random, string\n",
    "from typing import Optional, Union\n",
    "from dataclasses import dataclass\n",
    "# import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "# !pip install sentencepiece\n",
    "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "import wandb\n",
    "## run `wandb login --relogin`` in TERMINAL if you want to use your own  wandb profile\n",
    "\n",
    "\n",
    "print(f\"unsing {torch.device('cuda' if torch.cuda.is_available() else 'cpu')} with {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, indices=None):\n",
    "        \n",
    "        if indices is None:     ## when testing, not training, use the whole data without sampling indices\n",
    "            indices = range(len(data))\n",
    "            \n",
    "        self.data = data.iloc[indices]\n",
    "        self.hyps = self.data.filter(regex='hyp').keys()\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "    def __len__(self)  :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # obs1 sentences: questions that work as given sentence for multiple choice\n",
    "        obs1_sentences = [self.data.iloc[idx]['OBS1']] * len(self.hyps)\n",
    "        # hyp_obs2 sentences: option for multiple choices. obs2 sentece is fixed according to obs1 sentence. only hyppthesis sentences differs.\n",
    "        obs2_sentence = self.data.iloc[idx]['OBS2']\n",
    "        hyp_obs2_sentences = [f\"{self.data.iloc[idx][hyp]} {obs2_sentence}\" for hyp in self.hyps]\n",
    "\n",
    "        # text-encoding (tokenizing)\n",
    "        text_embed = self.tokenizer(obs1_sentences, hyp_obs2_sentences, padding=True, return_tensors='pt')\n",
    "        input_ids = text_embed['input_ids']\n",
    "        token_type_ids = text_embed['token_type_ids']\n",
    "        attention_mask = text_embed['attention_mask']\n",
    "        \n",
    "        # answers - 0, 1, 2, ..\n",
    "        label = torch.tensor(self.data.iloc[idx]['label']).unsqueeze(0)\n",
    "        \n",
    "        return {'input_ids' : input_ids, \n",
    "                'token_type_ids':token_type_ids, \n",
    "                'attention_mask' : attention_mask, \n",
    "                'labels':label } \n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase \n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "\n",
    "        # flattnen all the inputs/attetions masks etc.\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        # This will return a dictionary with tensors of shape `(batch_size * 4) x seq_length`\n",
    "        batch = self.tokenizer.pad(\n",
    "                flattened_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=\"pt\"\n",
    "                )\n",
    "        \n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "def get_tokenizer(text_model: str):\n",
    "    if \"kobert\" in text_model:\n",
    "        # !pip install sentencepiece\n",
    "        # !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "        from kobert_tokenizer import KoBERTTokenizer\n",
    "        tokenizer = KoBERTTokenizer.from_pretrained(text_model)\n",
    "    elif \"klue\" in text_model:\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(text_model, use_fast=True)\n",
    "    else:\n",
    "        print(\"Got unexpected text model and load AutoTokenizer. Please Check the tokenizer model\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(text_model, use_fast=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldTrainer :\n",
    "    def __init__(self, train_set: pd.DataFrame, test_set:pd.DataFrame, fold_idx: int, tokenizer, configs, args) :\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.args = args\n",
    "        \n",
    "        self.model = configs['text_model']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_collator = DataCollatorForMultipleChoice(tokenizer)\n",
    "\n",
    "\n",
    "        self.k = configs['kfolds']\n",
    "        self.kfold_split = list(KFold(n_splits=self.k, shuffle=True, random_state=42).split(train_set))\n",
    "        train_idx, val_idx = self.kfold_split[fold_idx]\n",
    "        \n",
    "        self.dataset_dict = {'train': TextDataset(self.train_set, self.tokenizer, train_idx), \n",
    "                       'valid' : TextDataset(self.train_set, self.tokenizer, val_idx),\n",
    "                       'test': TextDataset(self.test_set, self.tokenizer)}        \n",
    "\n",
    "        self.model = AutoModelForMultipleChoice.from_pretrained(self.model)\n",
    "        \n",
    "        wandb.watch(self.model)\n",
    "        wandb.config.update(self.args)        \n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model = self.model,\n",
    "            args = self.args,\n",
    "            train_dataset = self.dataset_dict['train'],\n",
    "            eval_dataset = self.dataset_dict['valid'],\n",
    "            data_collator = self.data_collator,\n",
    "            compute_metrics = self.compute_metrics,  \n",
    "            )        \n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, eval_predictions : transformers.EvalPrediction):\n",
    "        predictions, label_ids = eval_predictions\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "          \n",
    "    def train_fold(self, fold_idx):\n",
    "        # logging.info(f'{fold}/{self.k} - fold started')\n",
    "        print(f'===== {fold_idx+1}/{self.k} - fold TRAINING started =====')\n",
    "\n",
    "        self.model.train()\n",
    "        self.trainer.train()\n",
    "        \n",
    "        print(f'===== {fold_idx+1}/{self.k} - fold TESTING started =====')\n",
    "        self.model.eval()\n",
    "        metrics = self.trainer.evaluate(self.dataset_dict['test'])\n",
    "        print(f'== {fold_idx+1}th fold metric is {metrics} ==')\n",
    "                \n",
    "        del self.dataset_dict       ## re-sample datasets with another k-fold indices\n",
    "        torch.cuda.empty_cache()    ## empty CUDA memory before starting next fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !! CHECK !! the output_dir in main() function !!\n",
    "configs = {\n",
    "    'text_model' : 'klue/roberta-large',\n",
    "    'kfolds' : 3,\n",
    "    'train_path' : '/kovar-vol/kovar/dataset/train.json',\n",
    "    'test_path':'/kovar-vol/kovar/dataset/photo_test.json'\n",
    "}\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = f\"{configs['text_model']}-ft_base\",    ## this is just fundamental output_dir. It should change at main() to prevent overwrite\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy = \"epoch\", #evaluation is done (and logged) every eval_steps\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    logging_strategy = \"steps\", #logging is done every logging steps\n",
    "    learning_rate = 1e-6,\n",
    "    logging_steps = 500, #number of update steps between two logs if logging_strategy = \"steps\"\n",
    "    eval_steps = 500,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 5, \n",
    "    report_to = \"wandb\",\n",
    "    weight_decay = 0.01, #The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load datasets   \n",
    "    train_set = pd.read_json(configs['train_path'], lines=True)\n",
    "    test_set = pd.read_json(configs['test_path'], lines=True)\n",
    "\n",
    "    # Load Tokenizer\n",
    "    tokenizer = get_tokenizer(configs['text_model'])\n",
    "    \n",
    "    wandb.init(project='sage-sky-3')\n",
    "    \n",
    "    ## !! 혹 중간에 학습이 끊기면, ~ in [2,3] 이런 식으로 fold_idx를 조정해서 학습을 이어가자 !!\n",
    "    for fold_idx in [2]:\n",
    "        torch.cuda.empty_cache()   \n",
    "        args.output_dir = f\"/kovar-vol/kovar/models/klueRoBERTa_ft/klueROBERTa_ft-fold_{fold_idx+1}\"\n",
    "        kfold_trainer = KFoldTrainer(train_set, test_set, fold_idx, tokenizer, configs, args)\n",
    "        kfold_trainer.train_fold(fold_idx)\n",
    "        \n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrkfcl226\u001b[0m (\u001b[33mkovar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kovar-vol/kovar/Language-only/wandb/run-20230825_034015-xmnzltbk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kovar/sage-sky-3/runs/xmnzltbk' target=\"_blank\">grateful-breeze-3</a></strong> to <a href='https://wandb.ai/kovar/sage-sky-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kovar/sage-sky-3' target=\"_blank\">https://wandb.ai/kovar/sage-sky-3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kovar/sage-sky-3/runs/xmnzltbk' target=\"_blank\">https://wandb.ai/kovar/sage-sky-3/runs/xmnzltbk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForMultipleChoice: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 3/3 - fold TRAINING started =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2310/2310 2:06:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.385426</td>\n",
       "      <td>0.856330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672700</td>\n",
       "      <td>0.302593</td>\n",
       "      <td>0.888046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.275785</td>\n",
       "      <td>0.892383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.268533</td>\n",
       "      <td>0.897804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.267413</td>\n",
       "      <td>0.897262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 3/3 - fold TESTING started =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 01:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 3th fold metric is {'eval_loss': 0.24344861507415771, 'eval_accuracy': 0.9235197305679321, 'eval_runtime': 95.4465, 'eval_samples_per_second': 12.74, 'eval_steps_per_second': 0.796, 'epoch': 5.0} ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▅▅▅█</td></tr><tr><td>eval/loss</td><td>█▄▃▂▂▁</td></tr><tr><td>eval/runtime</td><td>▅████▁</td></tr><tr><td>eval/samples_per_second</td><td>█▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▇███</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.92352</td></tr><tr><td>eval/loss</td><td>0.24345</td></tr><tr><td>eval/runtime</td><td>95.4465</td></tr><tr><td>eval/samples_per_second</td><td>12.74</td></tr><tr><td>eval/steps_per_second</td><td>0.796</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>2310</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2652</td></tr><tr><td>train/total_flos</td><td>1.2336022964149542e+16</td></tr><tr><td>train/train_loss</td><td>0.38247</td></tr><tr><td>train/train_runtime</td><td>7605.8601</td></tr><tr><td>train/train_samples_per_second</td><td>4.851</td></tr><tr><td>train/train_steps_per_second</td><td>0.304</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-breeze-3</strong> at: <a href='https://wandb.ai/kovar/sage-sky-3/runs/xmnzltbk' target=\"_blank\">https://wandb.ai/kovar/sage-sky-3/runs/xmnzltbk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230825_034015-xmnzltbk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
