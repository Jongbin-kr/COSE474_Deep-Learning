{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsing cuda with 1.10.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import argparse, sys, os, random, string\n",
    "from typing import Optional, Union\n",
    "from dataclasses import dataclass\n",
    "# import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "# !pip install sentencepiece\n",
    "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "\n",
    "import wandb\n",
    "## run `wandb login --relogin`` in TERMINAL if you want to use your own  wandb profile\n",
    "\n",
    "\n",
    "print(f\"unsing {torch.device('cuda' if torch.cuda.is_available() else 'cpu')} with {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer, indices=None):\n",
    "        \n",
    "        if indices is None:     ## when testing, not training, use the whole data without sampling indices\n",
    "            indices = range(len(data))\n",
    "            \n",
    "        self.data = data.iloc[indices]\n",
    "        self.hyps = self.data.filter(regex='hyp').keys()\n",
    "        self.tokenizer=tokenizer\n",
    "        \n",
    "    def __len__(self)  :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # obs1 sentences: questions that work as given sentence for multiple choice\n",
    "        obs1_sentences = [self.data.iloc[idx]['OBS1']] * len(self.hyps)\n",
    "        # hyp_obs2 sentences: option for multiple choices. obs2 sentece is fixed according to obs1 sentence. only hyppthesis sentences differs.\n",
    "        obs2_sentence = self.data.iloc[idx]['OBS2']\n",
    "        hyp_obs2_sentences = [f\"{self.data.iloc[idx][hyp]} {obs2_sentence}\" for hyp in self.hyps]\n",
    "\n",
    "        # text-encoding (tokenizing)\n",
    "        text_embed = self.tokenizer(obs1_sentences, hyp_obs2_sentences, padding=True, return_tensors='pt')\n",
    "        input_ids = text_embed['input_ids']\n",
    "        token_type_ids = text_embed['token_type_ids']\n",
    "        attention_mask = text_embed['attention_mask']\n",
    "        \n",
    "        # answers - 0, 1, 2, ..\n",
    "        label = torch.tensor(self.data.iloc[idx]['label']).unsqueeze(0)\n",
    "        \n",
    "        return {'input_ids' : input_ids, \n",
    "                'token_type_ids':token_type_ids, \n",
    "                'attention_mask' : attention_mask, \n",
    "                'labels':label } \n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "    tokenizer: PreTrainedTokenizerBase \n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "\n",
    "        # flattnen all the inputs/attetions masks etc.\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        # This will return a dictionary with tensors of shape `(batch_size * 4) x seq_length`\n",
    "        batch = self.tokenizer.pad(\n",
    "                flattened_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=\"pt\"\n",
    "                )\n",
    "        \n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "def get_tokenizer(text_model: str):\n",
    "    if \"kobert\" in text_model:\n",
    "        # !pip install sentencepiece\n",
    "        # !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
    "        from kobert_tokenizer import KoBERTTokenizer\n",
    "        tokenizer = KoBERTTokenizer.from_pretrained(text_model)\n",
    "    elif \"klue\" in text_model:\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(text_model, use_fast=True)\n",
    "    else:\n",
    "        print(\"Got unexpected text model and load AutoTokenizer. Please Check the tokenizer model\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(text_model, use_fast=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldTrainer :\n",
    "    def __init__(self, train_set: pd.DataFrame, test_set:pd.DataFrame, fold_idx: int, tokenizer, configs, args) :\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.args = args\n",
    "        \n",
    "        self.model = configs['text_model']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_collator = DataCollatorForMultipleChoice(tokenizer)\n",
    "\n",
    "\n",
    "        self.k = configs['kfolds']\n",
    "        self.kfold_split = list(KFold(n_splits=self.k, shuffle=True, random_state=42).split(train_set))\n",
    "        train_idx, val_idx = self.kfold_split[fold_idx]\n",
    "        \n",
    "        self.dataset_dict = {'train': TextDataset(self.train_set, self.tokenizer, train_idx), \n",
    "                       'valid' : TextDataset(self.train_set, self.tokenizer, val_idx),\n",
    "                       'test': TextDataset(self.test_set, self.tokenizer)}        \n",
    "\n",
    "        self.model = AutoModelForMultipleChoice.from_pretrained(self.model)\n",
    "        \n",
    "        wandb.watch(self.model)\n",
    "        wandb.config.update(self.args)        \n",
    "        \n",
    "        self.trainer = Trainer(\n",
    "            model = self.model,\n",
    "            args = self.args,\n",
    "            train_dataset = self.dataset_dict['train'],\n",
    "            eval_dataset = self.dataset_dict['valid'],\n",
    "            data_collator = self.data_collator,\n",
    "            compute_metrics = self.compute_metrics,  \n",
    "            )        \n",
    "        \n",
    "        \n",
    "    def compute_metrics(self, eval_predictions : transformers.EvalPrediction):\n",
    "        predictions, label_ids = eval_predictions\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "          \n",
    "    def train_fold(self, fold_idx):\n",
    "        # logging.info(f'{fold}/{self.k} - fold started')\n",
    "        print(f'===== {fold_idx+1}/{self.k} - fold TRAINING started =====')\n",
    "\n",
    "        self.model.train()\n",
    "        self.trainer.train()\n",
    "        \n",
    "        print(f'===== {fold_idx+1}/{self.k} - fold TESTING started =====')\n",
    "        self.model.eval()\n",
    "        metrics = self.trainer.evaluate(self.dataset_dict['test'])\n",
    "        print(f'== {fold_idx+1}th fold metric is {metrics} ==')\n",
    "                \n",
    "        del self.dataset_dict       ## re-sample datasets with another k-fold indices\n",
    "        torch.cuda.empty_cache()    ## empty CUDA memory before starting next fold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !! CHECK !! the output_dir in main() function !!\n",
    "configs = {\n",
    "    'text_model' : 'skt/kobert-base-v1',\n",
    "    'kfolds' : 3,\n",
    "    'train_path' : '/kovar-vol/kovar/dataset/train.json',\n",
    "    'test_path':'/kovar-vol/kovar/dataset/photo_test.json'\n",
    "}\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = f\"{configs['text_model']}-ft_base\",    ## this is just fundamental output_dir. It should change at main() to prevent overwrite\n",
    "    overwrite_output_dir = False,\n",
    "    evaluation_strategy = \"epoch\", #evaluation is done (and logged) every eval_steps\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    logging_strategy = \"steps\", #logging is done every logging steps\n",
    "    learning_rate = 1e-6,\n",
    "    logging_steps = 500, #number of update steps between two logs if logging_strategy = \"steps\"\n",
    "    eval_steps = 500,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 5, \n",
    "    report_to = \"wandb\",\n",
    "    weight_decay = 0.01, #The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load datasets   \n",
    "    train_set = pd.read_json(configs['train_path'], lines=True)\n",
    "    test_set = pd.read_json(configs['test_path'], lines=True)\n",
    "\n",
    "    # Load Tokenizer\n",
    "    tokenizer = get_tokenizer(configs['text_model'])\n",
    "    \n",
    "    wandb.init()\n",
    "    \n",
    "    ## !! 혹 중간에 학습이 끊기면, range(2,3) 이런 식으로 fold_idx를 조정해서 학습을 이어가자 !!\n",
    "    for fold_idx in range(configs['kfolds']):   \n",
    "        args.output_dir = f\"/kovar-vol/kovar/models/koBERT_ft/koBERT_ft-fold_{fold_idx+1}\"\n",
    "        kfold_trainer = KFoldTrainer(train_set, test_set, fold_idx, tokenizer, configs, args)\n",
    "        kfold_trainer.train_fold(fold_idx)\n",
    "        \n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrkfcl226\u001b[0m (\u001b[33mkovar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kovar-vol/kovar/Language-only/wandb/run-20230824_034536-jjauuct7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kovar/kovar-Language-only/runs/jjauuct7' target=\"_blank\">rare-sponge-1</a></strong> to <a href='https://wandb.ai/kovar/kovar-Language-only' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kovar/kovar-Language-only' target=\"_blank\">https://wandb.ai/kovar/kovar-Language-only</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kovar/kovar-Language-only/runs/jjauuct7' target=\"_blank\">https://wandb.ai/kovar/kovar-Language-only/runs/jjauuct7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 1/3 - fold TRAINING started =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2310/2310 30:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838621</td>\n",
       "      <td>0.613550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>0.760526</td>\n",
       "      <td>0.657724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.800400</td>\n",
       "      <td>0.728964</td>\n",
       "      <td>0.674797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>0.710971</td>\n",
       "      <td>0.682656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.703673</td>\n",
       "      <td>0.687263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 1/3 - fold TESTING started =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 1th fold metric is {'eval_loss': 0.7144871950149536, 'eval_accuracy': 0.6735197305679321, 'eval_runtime': 30.5858, 'eval_samples_per_second': 39.757, 'eval_steps_per_second': 2.485, 'epoch': 5.0} ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 2/3 - fold TRAINING started =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2310/2310 45:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.838288</td>\n",
       "      <td>0.595554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.949400</td>\n",
       "      <td>0.774014</td>\n",
       "      <td>0.624017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.741688</td>\n",
       "      <td>0.650854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.759600</td>\n",
       "      <td>0.726926</td>\n",
       "      <td>0.666034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.738200</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>0.668203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 2/3 - fold TESTING started =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 2th fold metric is {'eval_loss': 0.7215868234634399, 'eval_accuracy': 0.6710526347160339, 'eval_runtime': 30.5661, 'eval_samples_per_second': 39.783, 'eval_steps_per_second': 2.486, 'epoch': 5.0} ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 3/3 - fold TRAINING started =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2310/2310 45:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839068</td>\n",
       "      <td>0.616969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.947100</td>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.644077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.732778</td>\n",
       "      <td>0.654107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.757600</td>\n",
       "      <td>0.714738</td>\n",
       "      <td>0.663323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.735800</td>\n",
       "      <td>0.710654</td>\n",
       "      <td>0.666034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 3/3 - fold TESTING started =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== 3th fold metric is {'eval_loss': 0.7198613882064819, 'eval_accuracy': 0.671875, 'eval_runtime': 30.5899, 'eval_samples_per_second': 39.752, 'eval_steps_per_second': 2.484, 'epoch': 5.0} ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▂▆▇██▇▁▃▅▆▇▇▃▅▅▆▆▇</td></tr><tr><td>eval/loss</td><td>█▄▂▁▁▂█▅▃▂▂▂█▄▃▂▁▂</td></tr><tr><td>eval/runtime</td><td>▂▂▅██▁█████▁█████▁</td></tr><tr><td>eval/samples_per_second</td><td>█▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▇███▁▁▃▃▅▅▆▇███▁▁▃▃▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▇███▁▁▃▃▅▅▆▇███▁▁▃▃▅▅▆▇███</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁█▆▃▁█▆▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▇▃▂▁▇▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁█▁</td></tr><tr><td>train/train_loss</td><td>▁█▇</td></tr><tr><td>train/train_runtime</td><td>▁██</td></tr><tr><td>train/train_samples_per_second</td><td>█▁▁</td></tr><tr><td>train/train_steps_per_second</td><td>█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.67188</td></tr><tr><td>eval/loss</td><td>0.71986</td></tr><tr><td>eval/runtime</td><td>30.5899</td></tr><tr><td>eval/samples_per_second</td><td>39.752</td></tr><tr><td>eval/steps_per_second</td><td>2.484</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>2310</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7358</td></tr><tr><td>train/total_flos</td><td>3698461342361952.0</td></tr><tr><td>train/train_loss</td><td>0.8013</td></tr><tr><td>train/train_runtime</td><td>2748.4173</td></tr><tr><td>train/train_samples_per_second</td><td>13.424</td></tr><tr><td>train/train_steps_per_second</td><td>0.84</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-sponge-1</strong> at: <a href='https://wandb.ai/kovar/kovar-Language-only/runs/jjauuct7' target=\"_blank\">https://wandb.ai/kovar/kovar-Language-only/runs/jjauuct7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230824_034536-jjauuct7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
