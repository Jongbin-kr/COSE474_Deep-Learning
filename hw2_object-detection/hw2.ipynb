{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hw2: Single-shot Multibox Object Detection\n",
    "2018131605 원종빈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install d2l\n",
    "# !git clone https://github.com/MLman/d2l-pytorch.git\n",
    "\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "from d2l import torch as d2l\n",
    "# from d2l.ssd_utils import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_predictor(input_channels, num_anchors, num_classes):\n",
    "    return nn.Conv2d(in_channels=input_channels, out_channels=num_anchors * (num_classes + 1), kernel_size=3,\n",
    "                     padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_predictor(input_channels, num_anchors):\n",
    "    return nn.Conv2d(in_channels=input_channels, out_channels=num_anchors * 4, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(x, block):\n",
    "    return block(x)\n",
    "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
    "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
    "Y1.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_pred(pred):\n",
    "    return torch.flatten(pred.permute(0,2,3,1), start_dim=1)\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return torch.cat([flatten_pred(p) for p in preds], dim=1)\n",
    "\n",
    "concat_preds([Y1, Y2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def down_sample_blk(in_channels, out_channels):\n",
    "    blk = []\n",
    "    for _ in range(2):\n",
    "        blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        blk.append(nn.BatchNorm2d(out_channels))\n",
    "        blk.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "\n",
    "    blk.append(nn.MaxPool2d(2, 2))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "forward(torch.zeros((2,3,20,20)), down_sample_blk(3, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def base_net():\n",
    "    blk = []\n",
    "    num_filters = [3, 16, 32, 64]\n",
    "    for i in range(len(num_filters)-1):\n",
    "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "forward(torch.zeros((2,3,256,256)), base_net()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complete model\n",
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 1:\n",
    "        blk = down_sample_blk(64, 128)\n",
    "    elif i == 4:\n",
    "        blk = nn.AdaptiveAvgPool2d((1,1))\n",
    "    else:\n",
    "        blk = down_sample_blk(128, 128)\n",
    "    return blk\n",
    "\n",
    "\n",
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X)\n",
    "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    bbox_preds = bbox_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, math\n",
    "\n",
    "def create_anchors(feature_map_sizes, steps, sizes):\n",
    "    scale = 256.\n",
    "    steps = [s / scale for s in steps]\n",
    "    sizes = [s / scale for s in sizes]\n",
    "    \n",
    "    aspect_ratios = ((2,),)  ## why use tuple in tuple? for multiple aspect_ratios?\n",
    "    \n",
    "    num_layers = len(feature_map_sizes)\n",
    "    boxes = []\n",
    "    for i in range(num_layers):\n",
    "        fmsize = feature_map_sizes[i]\n",
    "        for h, w in itertools.product(range(fmsize), repeat=2):\n",
    "            cx = (w + 0.5) * steps[i]\n",
    "            cy = (h + 0.5) * steps[i]\n",
    "            s = sizes[i]\n",
    "            boxes.append((cx, cy, s, s))\n",
    "            \n",
    "            s = sizes[i+1]\n",
    "            boxes.append((cx, cy, s, s))\n",
    "            \n",
    "            for ar in aspect_ratios[i]:\n",
    "                boxes.append((cx, cy, (s * math.sqrt(ar)), (s / math.sqrt(ar))))\n",
    "                boxes.append((cx, cy, (s / math.sqrt(ar)), (s * math.sqrt(ar))))\n",
    "    \n",
    "    return torch.Tensor(boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [[0.2*256, 0.272*256], [0.37*256, 0.447*256], [0.54*256, 0.619*256],\n",
    "         [0.71*256, 0.79*256], [0.88*256, 0.961*256]]\n",
    "ratios = [[1, 2, 0.5]] * 5\n",
    "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define TinySSD\n",
    "class TinySSD(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super().__init__()\n",
    "        input_channels_cls = 128\n",
    "        input_channels_bbox = 128\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.blk = []\n",
    "        self.cls = []\n",
    "        self.bbox = []\n",
    "        \n",
    "        self.blk_0 = get_blk(0)\n",
    "        self.blk_1 = get_blk(1)\n",
    "        self.blk_2 = get_blk(2)\n",
    "        self.blk_3 = get_blk(3)\n",
    "        self.blk_4 = get_blk(4)\n",
    "        \n",
    "        self.cls_0 = cls_predictor(64, num_anchors, num_classes)\n",
    "        self.cls_1 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
    "        self.cls_2 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
    "        self.cls_3 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
    "        self.cls_4 = cls_predictor(input_channels_cls, num_anchors, num_classes)\n",
    "        \n",
    "        self.bbox_0 = bbox_predictor(64, num_anchors)\n",
    "        self.bbox_1 = bbox_predictor(input_channels_bbox, num_anchors)\n",
    "        self.bbox_2 = bbox_predictor(input_channels_bbox, num_anchors)\n",
    "        self.bbox_3 = bbox_predictor(input_channels_bbox, num_anchors)\n",
    "        self.bbox_4 = bbox_predictor(input_channels_bbox, num_anchors)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        \n",
    "        X, anchors[0], cls_preds[0], bbox_preds[0] = blk_forward(X, self.blk_0, sizes[0], ratios[0], self.cls_0, self.bbox_0)\n",
    "        X, anchors[1], cls_preds[1], bbox_preds[1] = blk_forward(X, self.blk_1, sizes[1], ratios[1], self.cls_1, self.bbox_1)\n",
    "        X, anchors[2], cls_preds[2], bbox_preds[2] = blk_forward(X, self.blk_2, sizes[2], ratios[2], self.cls_2, self.bbox_2)\n",
    "        X, anchors[3], cls_preds[3], bbox_preds[3] = blk_forward(X, self.blk_3, sizes[3], ratios[3], self.cls_3, self.bbox_3)\n",
    "        X, anchors[4], cls_preds[4], bbox_preds[4] = blk_forward(X, self.blk_4, sizes[4], ratios[4], self.cls_4, self.bbox_4)\n",
    "        \n",
    "        print(anchors[0].shape)\n",
    "        print(anchors[1].shape)\n",
    "        print(anchors[2].shape)\n",
    "        print(anchors[3].shape)\n",
    "        print(anchors[4].shape)\n",
    "            \n",
    "        print(torch.cat(anchors, dim=0))\n",
    "        print(concat_preds(cls_preds).reshape((-1, 5444, self.num_classes + 1)))\n",
    "        print(concat_preds(bbox_preds))\n",
    "        \n",
    "        return (torch.cat(anchors, dim=0), concat_preds(cls_preds).reshape((-1, 5444, self.num_classes + 1)), concat_preds(bbox_preds))\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096, 4])\n",
      "torch.Size([1, 1024, 4])\n",
      "torch.Size([1, 256, 4])\n",
      "torch.Size([1, 64, 4])\n",
      "torch.Size([1, 4, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 4096 but got size 1024 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dieyo\\OneDrive - 고려대학교\\0_OBSIDIAN\\1_In_Box\\1_2023-2\\3_딥러닝_김현우\\hw\\hw2_object-detection\\hw2.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m net\u001b[39m.\u001b[39mapply(init_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m32\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m256\u001b[39m,\u001b[39m256\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m anchors, cls_preds, bbox_preds \u001b[39m=\u001b[39m net(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m anchors\u001b[39m.\u001b[39mshape, cls_preds\u001b[39m.\u001b[39mshape, bbox_preds\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\dieyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dieyo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\dieyo\\OneDrive - 고려대학교\\0_OBSIDIAN\\1_In_Box\\1_2023-2\\3_딥러닝_김현우\\hw\\hw2_object-detection\\hw2.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(anchors[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(anchors[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39;49mcat(anchors, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(concat_preds(cls_preds)\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m5444\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dieyo/OneDrive%20-%20%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90/0_OBSIDIAN/1_In_Box/1_2023-2/3_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EA%B9%80%ED%98%84%EC%9A%B0/hw/hw2_object-detection/hw2.ipynb#X20sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(concat_preds(bbox_preds))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 4096 but got size 1024 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "net = TinySSD(3, num_classes=1)\n",
    "net.apply(init_weights)\n",
    "\n",
    "X = torch.zeros((32,3,256,256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "anchors.shape, cls_preds.shape, bbox_preds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
